from ultralytics import YOLO
import cv2
import math 
import pyttsx3
import threading

# start webcam
cap = cv2.VideoCapture(0)
cap.set(3, 640)
cap.set(4, 480)

# Initialize the TTS engine
engine = pyttsx3.init()

# model
model = YOLO("yolo-Weights/yolov8n.pt")

# object classes
classNames = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck", "boat",
              "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat",
              "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella",
              "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat",
              "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup",
              "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli",
              "carrot", "hot dog", "pizza", "donut", "cake", "chair", "sofa", "pottedplant", "bed",
              "diningtable", "toilet", "tvmonitor", "laptop", "mouse", "remote", "keyboard", "cell phone",
              "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors",
              "teddy bear", "hair drier", "toothbrush"
              ]

# Define a function to handle TTS
def speak_async(text):

    def run_tts():
        engine = pyttsx3.init()  # Create a new instance of the TTS engine
        engine.setProperty('rate', 150)
        engine.setProperty('volume', 1.0)
        engine.say(text)
        engine.runAndWait()

    # Run TTS in a background thread
    tts_thread = threading.Thread(target=run_tts)
    tts_thread.start()


while True:
    
    success, img = cap.read()
    results = model(img, stream=True, verbose=False)
    #left zone
    cv2.rectangle(img, (100,200), (320,400) , (255, 0 , 0),3, 1)
    #right zone
    cv2.rectangle(img, (320, 200), (540,400), (255, 0 , 0),3, 1)



    # coordinates
    for r in results:
        boxes = r.boxes

        for box in boxes:
            # bounding box
            x1, y1, x2, y2 = box.xyxy[0]
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values
            confidence = math.ceil((box.conf[0]*100))/100

            if confidence >= 0.6:
                if x1 > 320 and x2 < 540 and y1 > 200 and y2 < 400:
                    # put box in cam
                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 2)
                    words = "In right zone , Move left"
                    # Say something
                    speak_async(words)
        
                    

                    
                elif x1 > 100 and x2 < 320 and y1 > 200 and y2 < 400:
                    # put box in camq
                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 2)
                    words = "In left zone , Move right"
                    # Say something
                    speak_async(words)
                    


                elif x1 < 100 and x2 > 540 and y2 > 400:
                    # put box in cam
                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 2)
                    words = "covering camera , Back up"
                    # Say something
                    speak_async(words)
                    


    cv2.imshow('Webcam', img)
    if cv2.waitKey(1) == ord('q'):
        break



cap.release()
cv2.destroyAllWindows()