from ultralytics import YOLO
import cv2
import math
import pyttsx3
import threading

# Initialize the TTS engine
engine = pyttsx3.init()
engine.setProperty('rate', 150)  # Set speech rate
engine.setProperty('volume', 1.0)  # Set volume

# Load the YOLO model
model = YOLO("yolo-Weights/session2/best.pt")

# Object classes
classNames = ["Automobile", "Obstacle", "Person", "Road", "Sidewalk"]

# Function to handle TTS
def speak_async(text):
    def run_tts():
        try:
            engine.say(text)
            engine.runAndWait()
        except RuntimeError as e:
            # Suppress "run loop already started" error
            if "run loop already started" in str(e):
                pass
            else:
                raise

    # Run TTS in a background thread
    tts_thread = threading.Thread(target=run_tts)
    tts_thread.start()

# Load video file
video_path = "C:\\Users\\Matt\\Desktop\\video3.mp4"  # Replace with your video file path
cap = cv2.VideoCapture(video_path)

# Check if the video file opened successfully
if not cap.isOpened():
    print("Error: Could not open video.")
    exit()

# Process the video
while cap.isOpened():
    success, img = cap.read()
    if not success:
        print("End of video or error reading frame.")
        break

    results = model(img, stream=True, verbose=False)

    # Left zone (for visualization only)
    cv2.rectangle(img, (100, 250), (360, 480), (255, 0, 0), 3, 1)
    # Right zone (for visualization only)
    cv2.rectangle(img, (360, 250), (620, 480), (255, 0, 0), 3, 1)

    # Coordinates and detections
    for r in results:
        boxes = r.boxes
        for box in boxes:
            # Bounding box
            x1, y1, x2, y2 = box.xyxy[0]
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)  # Convert to int
            confidence = math.ceil((box.conf[0] * 100)) / 100
            cls = int(box.cls[0])
            class_name = classNames[cls]

            # Detection threshold
            if confidence >= 0.5:
                # Right zone detection
                if (x1 > 360 and x2 < 620) and (y1 > 250 and y2 < 480) and (class_name != "Sidewalk" and class_name != "Road"):
                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 2)
                    words = class_name + " on right , Move left"
                    speak_async(words)
                    cv2.putText(img, class_name, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

                # Left zone detection
                elif (x1 > 100 and x2 < 360) and (y1 > 250 and y2 < 480) and (class_name != "Sidewalk" and class_name != "Road"):
                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 2)

                    words = class_name + " on left , Move right"
                    speak_async(words)
                    cv2.putText(img, class_name, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

    # Display the video frame
    cv2.imshow('Video', img)

    # Break on pressing 'q'
    if cv2.waitKey(1) == ord('q'):
        break

# Release resources 
cap.release()
cv2.destroyAllWindows()
